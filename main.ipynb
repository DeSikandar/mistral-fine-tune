{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"my-data-chat/\" \n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 16870.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle().select(range(150))\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "dataset = dataset.train_test_split(test_size=50/150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_27871460_2 (network VARCHAR, state_or_territory VARCHAR)', 'role': 'system'}, {'content': 'Which network was located in Illinois?', 'role': 'user'}, {'content': 'SELECT network FROM table_27871460_2 WHERE state_or_territory = \"Illinois\"', 'role': 'assistant'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 341.42ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1086.04ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1347.35ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_valid = dataset['test'].train_test_split(0.5)\n",
    "print(dataset[\"train\"][45][\"messages\"])\n",
    "dataset[\"train\"].to_json(folder + \"train.jsonl\", orient=\"records\")\n",
    "dataset_test_valid[\"train\"].to_json(folder + \"test.jsonl\", orient=\"records\")\n",
    "dataset_test_valid[\"test\"].to_json(folder + \"valid.jsonl\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.lora import  train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module mlx_lm.tuner.trainer:\n",
      "\n",
      "train(\n",
      "    model,\n",
      "    tokenizer,\n",
      "    optimizer,\n",
      "    train_dataset,\n",
      "    val_dataset,\n",
      "    args: mlx_lm.tuner.trainer.TrainingArgs = TrainingArgs(batch_size=4, iters=100, val_batches=25, steps_per_report=10, steps_per_eval=200, steps_per_save=100, max_seq_length=2048, adapter_file='adapters.safetensors', grad_checkpoint=False),\n",
      "    loss: <built-in function callable> = <function default_loss at 0x147255300>,\n",
      "    iterate_batches: <built-in function callable> = <function iterate_batches at 0x147255440>,\n",
      "    training_callback: mlx_lm.tuner.trainer.TrainingCallback = None\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'tokenizer' and 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-Instruct-v0.2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy-data-chat/train.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy-data-chat/valid.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43miterate_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'tokenizer' and 'optimizer'"
     ]
    }
   ],
   "source": [
    "# train(model='mistralai/Mistral-7B-Instruct-v0.2',train_dataset='my-data-chat/train.jsonl',val_dataset='my-data-chat/valid.jsonl' ,iterate_batches=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Fine tune the model need to crate the dataset according to model like for the mistral we can use the data \n",
    "\n",
    "{\"messages\":[{\"content\":\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_97 (tie_no VARCHAR, away_team VARCHAR)\",\"role\":\"system\"},{\"content\":\"How many ties does Walsall have?\",\"role\":\"user\"},{\"content\":\"SELECT tie_no FROM table_name_97 WHERE away_team = \\\"walsall\\\"\",\"role\":\"assistant\"}]}\n",
    "\n",
    "with jsonl file \n",
    "\n",
    "\n",
    "\n",
    "we have seprate file for the train.jsonl and valid.jsonl and test.jsonl\n",
    "\n",
    "I am using the mlx libary to train my model this mlx libary supported by the apple silicaon chip \n",
    "\n",
    "For that we need to install the library \n",
    "\n",
    "\n",
    "pip install mlx_lm \n",
    "\n",
    "\n",
    "For the download the model using the command line \n",
    "\n",
    "\n",
    "\n",
    "mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt \"hello\"\n",
    "\n",
    "it will download model from the hugginface and store it in to locally \n",
    "\n",
    "\n",
    "For the training the we use the dataset we created \n",
    "\n",
    "and then we can run the traing using the command \n",
    "\n",
    "mlx_lm.lora --train --model mistralai/Mistral-7B-Instruct-v0.2 --data my-data-chat --batch-size 2\n",
    "\n",
    "\n",
    "it will take some time and then it will train the model \n",
    "\n",
    "it will create the adapters folder in that we have weight for the model using this weight we can creat the ollam model and then use this model locally \n",
    "\n",
    "\n",
    "here is setups \n",
    "\n",
    "1. create the Modelfile \n",
    "    FROM mistral //model name\n",
    "    ADAPTER ./adapters //adapter location \n",
    "\n",
    "2. run ollam command to create the model \n",
    "\n",
    "    ollama create <sqlmistral> -f Modelfile\n",
    "\n",
    "3. run newly created model locally \n",
    "    ollama run sqlmistral\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
